{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5387d2",
   "metadata": {},
   "source": [
    "# 2. Прототипирование Модели Распознавания\n",
    "\n",
    "Этот ноутбук используется для создания экземпляров компонентов модели (`EncoderCNN`, `Attention`, `DecoderRNN`, `RecognitionModel`) и проверки их работы на фиктивных данных, чтобы убедиться в правильности размерностей и соединений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f12222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Добавляем корневую директорию проекта в путь\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Импортируем всю модель\n",
    "from src.recognition.model import RecognitionModel, EncoderCNN # Импортируем и Encoder для примера"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4235f03",
   "metadata": {},
   "source": [
    "## Параметры Модели и Фиктивных Данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c8b6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примерные длины подписей (без <start>): [12, 39, 35, 43]\n"
     ]
    }
   ],
   "source": [
    "# Параметры модели (должны совпадать с теми, что в model.py или передаваться при создании)\n",
    "EMBED_DIM = 256\n",
    "DECODER_DIM = 512\n",
    "VOCAB_SIZE = 100  # Примерный размер словаря (включая PAD, SOS, EOS)\n",
    "ATTENTION_DIM = 512\n",
    "ENCODER_FEATURES = 512\n",
    "DROPOUT = 0.5\n",
    "INPUT_CHANNELS = 1 # Серое изображение\n",
    "\n",
    "# Параметры фиктивного входа\n",
    "BATCH_SIZE = 4\n",
    "IMG_HEIGHT = 64   # Высота изображения после предобработки\n",
    "IMG_WIDTH = 256   # Ширина изображения после предобработки\n",
    "MAX_CAPTION_LEN = 50 # Максимальная длина подписи (включая SOS, EOS, PAD)\n",
    "\n",
    "# Примерные длины подписей для батча (без токена <start>)\n",
    "# Длины должны быть меньше или равны MAX_CAPTION_LEN\n",
    "dummy_lengths = torch.randint(10, MAX_CAPTION_LEN, (BATCH_SIZE,), dtype=torch.long)\n",
    "# Убедимся, что caption_lengths > 0\n",
    "dummy_lengths = torch.clamp(dummy_lengths, min=1)\n",
    "print(f\"Примерные длины подписей (без <start>): {dummy_lengths.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3c0a4",
   "metadata": {},
   "source": [
    "## Тестирование EncoderCNN (Повторно, для справки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b9cc59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Форма выхода EncoderCNN: torch.Size([4, 512, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderCNN(input_channels=INPUT_CHANNELS, output_features=ENCODER_FEATURES)\n",
    "dummy_images = torch.randn(BATCH_SIZE, INPUT_CHANNELS, IMG_HEIGHT, IMG_WIDTH)\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    features_test = encoder(dummy_images)\n",
    "print(f\"Форма выхода EncoderCNN: {features_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba4b42d",
   "metadata": {},
   "source": [
    "## Тестирование Полной Модели (RecognitionModel)\n",
    "\n",
    "Создадим экземпляр `RecognitionModel` и прогоним через него фиктивные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a8aec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Инициализирован Attention (Bahdanau-style)\n",
      "Инициализирован DecoderRNN с LSTMCell (decoder_dim=512)\n",
      "\n",
      "Форма входных изображений: torch.Size([4, 1, 64, 256])\n",
      "Форма входных подписей: torch.Size([4, 50])\n",
      "Длины подписей (без <start>): [12, 39, 35, 43]\n",
      "\n",
      "Форма выходного тензора предсказаний: torch.Size([4, 42, 100])\n",
      "Ожидаемая форма: (4, 42, 100)\n"
     ]
    }
   ],
   "source": [
    "# Создаем полную модель\n",
    "model = RecognitionModel(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    decoder_dim=DECODER_DIM,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    encoder_output_features=ENCODER_FEATURES,\n",
    "    attention_dim=ATTENTION_DIM,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "# print(model) # Можно раскомментировать, чтобы увидеть структуру\n",
    "\n",
    "# Создаем фиктивные входные данные\n",
    "dummy_images = torch.randn(BATCH_SIZE, INPUT_CHANNELS, IMG_HEIGHT, IMG_WIDTH)\n",
    "\n",
    "# Фиктивные подписи (должны начинаться с токена <start>, например, индекс 1)\n",
    "# Заполняем случайными индексами от 1 до VOCAB_SIZE-1 (исключая PAD=0, SOS=1, EOS=2)\n",
    "dummy_captions = torch.randint(3, VOCAB_SIZE, (BATCH_SIZE, MAX_CAPTION_LEN), dtype=torch.long)\n",
    "# Устанавливаем токен <start> (предположим, индекс 1) в начале каждой подписи\n",
    "dummy_captions[:, 0] = 1 \n",
    "# Важно: caption_lengths используются для цикла декодера, они не включают <start>\n",
    "# Поэтому нам нужны сами фиктивные подписи для teacher forcing\n",
    "\n",
    "print(f\"\\nФорма входных изображений: {dummy_images.shape}\")\n",
    "print(f\"Форма входных подписей: {dummy_captions.shape}\")\n",
    "print(f\"Длины подписей (без <start>): {dummy_lengths.tolist()}\")\n",
    "\n",
    "# Прогоняем данные через модель\n",
    "try:\n",
    "    # Переводим модель в режим оценки\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Передаем подписи без последнего токена, т.к. его предсказываем\n",
    "        predictions = model(dummy_images, dummy_captions, dummy_lengths)\n",
    "    \n",
    "    # Выход декодера: (Batch, Max_Decode_Length, Vocab_Size)\n",
    "    # Max_Decode_Length = max(dummy_lengths) - 1\n",
    "    print(f\"\\nФорма выходного тензора предсказаний: {predictions.shape}\")\n",
    "    print(f\"Ожидаемая форма: ({BATCH_SIZE}, {max(l - 1 for l in dummy_lengths.tolist())}, {VOCAB_SIZE})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nОшибка при выполнении model.forward(): {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc() # Печатаем полный traceback для отладки"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
